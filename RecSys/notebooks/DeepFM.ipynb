{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2834cbe0-e27a-4649-9846-737e8eb87106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ptrbelonovsky/Desktop/DSBA4YEAR/KRIS/RecSys_thesis/recsysvenv/bin/python3.12'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DO NOT ERASE THIS. IMPORTANT TO CORRECTLY IMPORT MODULES\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d08ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy.sparse.linalg import svds\n",
    "# from surprise import SVDpp, Reader, Dataset\n",
    "# from surprise.model_selection import cross_validate\n",
    "\n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "from src.metrics import reccomendation_report\n",
    "from src.utils import surprise_predict\n",
    "from src.utils import split_test_df\n",
    "from src.utils import seed_everything\n",
    "from src.utils import trainDataset\n",
    "from src.utils import trainDatasetWithNumCatFeatures\n",
    "\n",
    "from src.models import DeepFM\n",
    "\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a032ae8",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de185e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../../data/ml-1m/\"\n",
    "RANDOM_STATE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1136b767-5777-4471-8136-d4a3cdeae7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0722360",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db9e06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv(\n",
    "    DATA_FOLDER + \"movies.csv\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    sep=\";\",\n",
    "    names=[\"movieId\", \"name\", \"genre\"],\n",
    ")\n",
    "df_ratings = pd.read_csv(\n",
    "    DATA_FOLDER + \"ratings.csv\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    sep=\";\",\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    ")\n",
    "df_users = pd.read_csv(\n",
    "    DATA_FOLDER + \"users.csv\",\n",
    "    encoding=\"iso-8859-1\",\n",
    "    sep=\";\",\n",
    "    names=[\"userId\", \"gender\", \"age\", \"occupation\", \"zip-code\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8af6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encode usedId, movieId\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "df_movies[\"movieId\"] = movie_encoder.fit_transform(df_movies[\"movieId\"])\n",
    "df_users[\"userId\"] = user_encoder.fit_transform(df_users[\"userId\"])\n",
    "\n",
    "df_ratings[\"movieId\"] = movie_encoder.transform(df_ratings[\"movieId\"])\n",
    "df_ratings[\"userId\"] = user_encoder.transform(df_ratings[\"userId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad0c97-ef07-4ea3-a562-2c3e6c9450cd",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "Methodology: Last user interaction is a test item. The rest is train. Validation part is 10% of test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "640bea58-f388-4778-8a1b-2622a636eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings[\"rank\"] = (\n",
    "    df_ratings[[\"userId\", \"timestamp\"]]\n",
    "    .groupby(\"userId\", as_index=False)[\"timestamp\"]\n",
    "    .rank(method=\"first\", ascending=False)\n",
    ")\n",
    "# df_ratings = df_ratings.merge(\n",
    "#     pd.DataFrame(df_ratings[\"userId\"].value_counts()).reset_index(),\n",
    "#     how=\"left\", on=\"userId\")\n",
    "# df_ratings[\"cum_position\"] = df_ratings[\"rank\"] / df_ratings[\"count\"]\n",
    "# df_ratings = df_ratings.drop(columns=[\"rank\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "457f8830-7393-446f-8e58-74c129e3196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave one out\n",
    "df_train = df_ratings.loc[df_ratings[\"rank\"] != 1].reset_index(drop=True)\n",
    "df_test = (\n",
    "    df_ratings.loc[df_ratings[\"rank\"] == 1].reset_index(drop=True).assign(action=1)\n",
    ")\n",
    "df_test, df_val = train_test_split(df_test, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8273b002-c246-4b04-b806-9b2236d7472d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6362bfe59d649a089f740986093d3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Enriching test:   0%|          | 0/4832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0768d011504ad18d489f7bc2fdba5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Enriching val:   0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# enrich test data with 100 random movies from the ones not intercated by user\n",
    "df_add = pd.DataFrame()\n",
    "for user in tqdm(df_test.userId.unique(), desc=\"Enriching test\"):\n",
    "    movie = df_test.loc[df_test.userId == user, \"movieId\"]\n",
    "    watched_movies = np.append(\n",
    "        movie, df_train.loc[df_train.userId == user, \"movieId\"].values\n",
    "    )\n",
    "    not_wathed_movies = np.setdiff1d(\n",
    "        np.arange(df_movies[\"movieId\"].max() + 1), watched_movies\n",
    "    )\n",
    "    random_100 = np.random.choice(not_wathed_movies, 100, replace=False)\n",
    "\n",
    "    df_temp = pd.DataFrame().assign(movieId=random_100, userId=user, action=0)\n",
    "    df_add = pd.concat([df_add, df_temp], ignore_index=True)\n",
    "\n",
    "df_test = pd.concat([df_test, df_add], ignore_index=True).drop(\n",
    "    columns=[\"timestamp\", \"rating\", \"rank\"]\n",
    ")\n",
    "\n",
    "df_add = pd.DataFrame()\n",
    "for user in tqdm(df_val.userId.unique(), desc=\"Enriching val\"):\n",
    "    movie = df_val.loc[df_val.userId == user, \"movieId\"]\n",
    "    watched_movies = np.append(\n",
    "        movie, df_train.loc[df_train.userId == user, \"movieId\"].values\n",
    "    )\n",
    "    not_wathed_movies = np.setdiff1d(\n",
    "        np.arange(df_movies[\"movieId\"].max() + 1), watched_movies\n",
    "    )\n",
    "    random_100 = np.random.choice(not_wathed_movies, 100, replace=False)\n",
    "\n",
    "    df_temp = pd.DataFrame().assign(movieId=random_100, userId=user, action=0)\n",
    "    df_add = pd.concat([df_add, df_temp], ignore_index=True)\n",
    "\n",
    "df_val = pd.concat([df_val, df_add], ignore_index=True).drop(\n",
    "    columns=[\"timestamp\", \"rating\", \"rank\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca3a59-2947-4594-8fd8-c1fdc20aebe6",
   "metadata": {},
   "source": [
    "## Building one-hot-encoded featurues and normalizing continious features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef1e4b7-df05-4595-aceb-2c647984e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_user = OrdinalEncoder()\n",
    "user_cat = torch.tensor(ord_user.fit_transform(df_users[[\"gender\", \"occupation\"]]))\n",
    "\n",
    "ord_movie = OrdinalEncoder()\n",
    "movie_cat = torch.tensor(ord_movie.fit_transform(df_movies[[\"genre\"]]))\n",
    "\n",
    "ss_user = StandardScaler()\n",
    "user_num = torch.tensor(ss_user.fit_transform(df_users[[\"age\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd76edec-9e33-48cc-8877-4c4ce329e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#     trainDatasetWithNumCatFeatures(\n",
    "#         df_train,\n",
    "#         df_movies[\"movieId\"].nunique(),\n",
    "#         user_cat,\n",
    "#         user_num,\n",
    "#         movie_cat\n",
    "#     ),\n",
    "#     batch_size=2048,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73a2f2bf-2f31-4480-8671-b9e67327fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_cat = user_cat[df_test[\"userId\"].values].clone().detach().to(torch.long)\n",
    "test_user_num = user_num[df_test[\"userId\"].values].clone().detach().to(torch.float)\n",
    "test_movie_cat = movie_cat[df_test[\"movieId\"].values].clone().detach().to(torch.long)\n",
    "test_cat = torch.hstack((test_user_cat, test_movie_cat))\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        torch.tensor(df_test[\"userId\"]),\n",
    "        torch.tensor(df_test[\"movieId\"]),\n",
    "        test_user_num,\n",
    "        test_cat,\n",
    "    ),\n",
    "    batch_size=4096,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "075b3104-639d-4381-959a-d40f24764e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('data/train_loader.pkl', \"wb\") as f:\n",
    "#     pickle.dump(train_loader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8621deb6-fff4-4618-94b1-cb560343d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT ERASE THIS. IMPORTANT TO CORRECTLY IMPORT MODULES\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.executable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy.sparse.linalg import svds\n",
    "# from surprise import SVDpp, Reader, Dataset\n",
    "# from surprise.model_selection import cross_validate\n",
    "\n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "from src.metrics import reccomendation_report\n",
    "from src.utils import surprise_predict\n",
    "from src.utils import split_test_df\n",
    "from src.utils import seed_everything\n",
    "from src.utils import trainDataset\n",
    "from src.utils import trainDatasetWithNumCatFeatures\n",
    "\n",
    "from src.models import DeepFM\n",
    "\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4959ebae-4a94-4a47-8830-6ff994ff607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/train_loader.pkl\", \"rb\") as f:\n",
    "    train_loader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec1527-f30b-4cc2-885c-9d1b94bc085b",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "880e8be0-c40a-4095-b551-273067cce92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 6040  # df_users[\"userId\"].nunique()\n",
    "num_items = 3883  # df_movies[\"movieId\"].nunique()\n",
    "num_numeric_feats = 1\n",
    "cat_feature_vocab = [\n",
    "    2,\n",
    "    21,\n",
    "    301,\n",
    "]  # [len(i) for i in ord_user.categories_] + [len(i) for i in ord_movie.categories_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2363a9b-3200-4bce-9bb9-6b5046547352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepFM(\n",
       "  (user_embedding): Embedding(6040, 5)\n",
       "  (item_embedding): Embedding(3883, 5)\n",
       "  (numerical_embeddings): ModuleList(\n",
       "    (0): Linear(in_features=1, out_features=5, bias=True)\n",
       "  )\n",
       "  (categorical_embeddings): ModuleList(\n",
       "    (0): Embedding(2, 5)\n",
       "    (1): Embedding(21, 5)\n",
       "    (2): Embedding(301, 5)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (MLP_layer_0): Linear(in_features=30, out_features=16, bias=True)\n",
       "    (Activation_layer_0): ReLU()\n",
       "    (MLP_layer_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (Activation_layer_1): ReLU()\n",
       "    (MLP_layer_2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (Activation_layer_2): ReLU()\n",
       "    (MLP_layer_3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (Activation_layer_3): ReLU()\n",
       "  )\n",
       "  (fm_linear): Linear(in_features=325, out_features=1, bias=True)\n",
       "  (final_sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dF = DeepFM(num_users, num_items, num_numeric_feats, cat_feature_vocab)\n",
    "display(dF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cbf91a5-facf-453a-8d94-52cc0913b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(dF.parameters(), lr=3e-4)\n",
    "criterion = nn.BCELoss()\n",
    "n_epochs = 1\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "803854c0-aa03-4958-a506-151f11d2f1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700c205da65241daae4c4688a65fc86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da82d35dc0f24ec49c87dd0479764b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2428 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1061a2ba0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ptrbelonovsky/Desktop/DSBA4YEAR/KRIS/RecSys_thesis/recsysvenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Train DeepFM\n",
    "dF.to(device)\n",
    "num_iterations = len(train_loader)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Epochs\"):\n",
    "    # train\n",
    "    total_train_loss = 0\n",
    "    dF.train()\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for userIds, movieIds, num_feats, cat_feats, ratings in tepoch:\n",
    "            pred_train = dF(\n",
    "                userIds.to(device),\n",
    "                movieIds.to(device),\n",
    "                num_feats.to(device),\n",
    "                cat_feats.to(device),\n",
    "            )\n",
    "            loss_train = criterion(pred_train.flatten(), ratings.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss_train.item()\n",
    "            tepoch.set_postfix(loss=loss_train.item())\n",
    "    print(\"Epoch:\", epoch)\n",
    "    print(\"Train loss\", round(total_train_loss / num_iterations, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c727b7e-8927-4143-9df7-590bf59cb391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8632a69adc46dfb940a38fc4d70982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/120 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict dF\n",
    "dF.eval()\n",
    "total_preds = torch.zeros(len(test_loader.dataset))\n",
    "batch_size = test_loader.batch_size\n",
    "for i, (userIds, movieIds, num_feats, cat_feats) in enumerate(\n",
    "    tqdm(test_loader, desc=\"Inference\", unit=\"batch\")\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        total_preds[i * batch_size : (i + 1) * batch_size] = dF(\n",
    "            userIds.to(device),\n",
    "            movieIds.to(device),\n",
    "            num_feats.to(device),\n",
    "            cat_feats.to(device),\n",
    "        ).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "163da7ef-593d-4519-ab43-a869e3ad6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"rating_pred\"] = total_preds.numpy()\n",
    "pred, target = split_test_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec7da670-3d47-4bec-8e49-f68ca2c09ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hit rate @ K': tensor(0.6329), 'NDCG @ K': tensor(0.1798)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reccomendation_report(pred, target, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2531280-3464-47f0-a540-3aa125675367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
